{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a04a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ars/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/ars/anaconda3/envs/rapids-25.08/lib/python3.12/site-packages/pymorphy3/analyzer.py:114: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.13)\n",
      "/home/ars/.cache/kagglehub/datasets/blackmoon/russian-language-toxic-comments/versions/1/labeled.csv\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from lemmatizer import get_normal_series\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy3 import MorphAnalyzer \n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "morph = MorphAnalyzer()\n",
    "stopwords = set(stopwords.words('russian'))\n",
    "\n",
    "path = kagglehub.dataset_download(\"blackmoon/russian-language-toxic-comments\")\n",
    "path = path + '/' + os.listdir(path)[0]\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4431204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нули comment    0\n",
      "toxic      0\n",
      "dtype: int64\n",
      "0.33 - доля токсиков\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_num_letters</th>\n",
       "      <th>mean_num_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>194</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>141</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_num_letters  mean_num_words\n",
       "toxic                                  \n",
       "0.0                 194              30\n",
       "1.0                 141              22"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path)\n",
    "print(\"Нули\", df.isna().sum())\n",
    "\n",
    "print(f\"{df['toxic'].mean():.2f} - доля токсиков\")\n",
    "\n",
    "df.groupby('toxic').agg(mean_num_letters = ('comment', lambda x: x.str.len().mean().astype(int)),\n",
    "                        mean_num_words = ('comment', lambda x: x.str.split().str.len().mean().astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c6ed42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ff5e6cb524494da6e35f95aa2cdc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1201), Label(value='0 / 1201'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Уникальных слов: 33809\n",
      "Слов с частотой больше 3: 8769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text: str) -> list[str]:\n",
    "    return ''.join(['е' if i.lower()=='ё' else \n",
    "                    i.lower() if i.isalpha() else \n",
    "                    ' ' for i in text]).split()\n",
    "\n",
    "def drop_stopwords(li: list[str], stopwords) -> list[str]:\n",
    "    return [i for i in li if i not in stopwords]\n",
    "\n",
    "df['comment'] = df['comment'].apply(tokenizer)\n",
    "df['comment'] = get_normal_series(df['comment'])\n",
    "df['comment'] = df['comment'].apply(lambda x: drop_stopwords(x, stopwords))\n",
    "\n",
    "li = []\n",
    "for i in df.comment.values:\n",
    "    li.extend(i)\n",
    "\n",
    "count = Counter(li)\n",
    "drop_level = 3\n",
    "print(f'''\n",
    "Уникальных слов: {len(count)}\n",
    "Слов с частотой больше {drop_level}: {sum([1 for i in count.values() if i > drop_level])}\n",
    "''')\n",
    "drop_words = [i for i in count if count[i] <= drop_level]\n",
    "used_words = [i for i in count if count[i] > drop_level]\n",
    "\n",
    "df['comment'] = df['comment'].apply(lambda x: drop_stopwords(x, set(drop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b652b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(words: list[str], documents: list[list[str]]) -> list[list[float]]:\n",
    "    '''\n",
    "    Вовращает tf в формате\n",
    "    shape: (len documents, len words)\n",
    "    '''\n",
    "\n",
    "    count_ = []\n",
    "    for text in documents:\n",
    "        count_.append(Counter(text))\n",
    "\n",
    "    tf = []\n",
    "    for text_count in count_:\n",
    "        tf.append([])\n",
    "        sum_ = sum(text_count.values())\n",
    "        sum_ = sum_ if sum_ > 0 else 1\n",
    "        for word in words:\n",
    "            tf[-1].append(text_count.get(word, 0) / sum_)\n",
    "    \n",
    "    return tf\n",
    "\n",
    "\n",
    "def get_idf(words: list[str], documents: list[list[str]]) -> list[float]:\n",
    "    '''\n",
    "    Возвращает idf в формате \n",
    "    shape: (len words)\n",
    "    '''\n",
    "    documents = [set(text) for text in documents]\n",
    "        \n",
    "    idf = []\n",
    "    for word in words:\n",
    "        count_ = 0\n",
    "        for text in documents:\n",
    "            count_ += word in text\n",
    "        idf.append(np.log((len(documents) + 1) / (count_ + 1)))\n",
    "\n",
    "    return idf\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13afe44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = np.array(get_tf(used_words, df.comment.values.tolist()))\n",
    "idf = np.array(get_idf(used_words, df.comment.values.tolist()))\n",
    "tf_idf = tf * idf.reshape([1, -1])\n",
    "\n",
    "# фичи - tf_idf\n",
    "# таргет - df['toxic']\n",
    "# слова - used_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d5d903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8414845646895595, 0.7424849699398798)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cuml.decomposition import PCA, TruncatedSVD\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "idf_train, idf_val, y_train, y_val = train_test_split(tf_idf, df['toxic'], test_size=0.2)\n",
    "\n",
    "# pca = PCA(n_components=100)\n",
    "# idf_train = pca.fit_transform(idf_train)\n",
    "# idf_val = pca.transform(idf_val)\n",
    "\n",
    "# svd = TruncatedSVD(n_components=1000)\n",
    "# idf_train = svd.fit_transform(idf_train)\n",
    "# idf_val = svd.transform(idf_val)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=300, max_depth=100)\n",
    "model.fit(idf_train, y_train)\n",
    "res = model.predict_proba(idf_val)\n",
    "\n",
    "res_ = [[int(i > drop_rate/100) for i in res[:,1]] for drop_rate in range(0, 100)]\n",
    "# accuracy_score(y_val, res_), f1_score(y_val, res_)\n",
    "max(map(lambda x: accuracy_score(y_val, x), res_)), max(map(lambda x: f1_score(y_val, x), res_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a49c5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11529, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
