{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be62212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ars/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/ars/anaconda3/envs/rapids-25.08/lib/python3.12/site-packages/pymorphy3/analyzer.py:114: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.13)\n",
      "/home/ars/.cache/kagglehub/datasets/blackmoon/russian-language-toxic-comments/versions/1/labeled.csv\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy3 import MorphAnalyzer \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "morph = MorphAnalyzer()\n",
    "stopwords = set(stopwords.words('russian'))\n",
    "\n",
    "path = kagglehub.dataset_download(\"blackmoon/russian-language-toxic-comments\")\n",
    "path = path + '/' + os.listdir(path)[0]\n",
    "print(path)\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\").to('cuda')\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700100c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/180\n",
      "2/180\n",
      "3/180\n",
      "4/180\n",
      "5/180\n",
      "6/180\n",
      "7/180\n",
      "8/180\n",
      "9/180\n",
      "10/180\n",
      "11/180\n",
      "12/180\n",
      "13/180\n",
      "14/180\n",
      "15/180\n",
      "16/180\n",
      "17/180\n",
      "18/180\n",
      "19/180\n",
      "20/180\n",
      "21/180\n",
      "22/180\n",
      "23/180\n",
      "24/180\n",
      "25/180\n",
      "26/180\n",
      "27/180\n",
      "28/180\n",
      "29/180\n",
      "30/180\n",
      "31/180\n",
      "32/180\n",
      "33/180\n",
      "34/180\n",
      "35/180\n",
      "36/180\n",
      "37/180\n",
      "38/180\n",
      "39/180\n",
      "40/180\n",
      "41/180\n",
      "42/180\n",
      "43/180\n",
      "44/180\n",
      "45/180\n",
      "46/180\n",
      "47/180\n",
      "48/180\n",
      "49/180\n",
      "50/180\n",
      "51/180\n",
      "52/180\n",
      "53/180\n",
      "54/180\n",
      "55/180\n",
      "56/180\n",
      "57/180\n",
      "58/180\n",
      "59/180\n",
      "60/180\n",
      "61/180\n",
      "62/180\n",
      "63/180\n",
      "64/180\n",
      "65/180\n",
      "66/180\n",
      "67/180\n",
      "68/180\n",
      "69/180\n",
      "70/180\n",
      "71/180\n",
      "72/180\n",
      "73/180\n",
      "74/180\n",
      "75/180\n",
      "76/180\n",
      "77/180\n",
      "78/180\n",
      "79/180\n",
      "80/180\n",
      "81/180\n",
      "82/180\n",
      "83/180\n",
      "84/180\n",
      "85/180\n",
      "86/180\n",
      "87/180\n",
      "88/180\n",
      "89/180\n",
      "90/180\n",
      "91/180\n",
      "92/180\n",
      "93/180\n",
      "94/180\n",
      "95/180\n",
      "96/180\n",
      "97/180\n",
      "98/180\n",
      "99/180\n",
      "100/180\n",
      "101/180\n",
      "102/180\n",
      "103/180\n",
      "104/180\n",
      "105/180\n",
      "106/180\n",
      "107/180\n",
      "108/180\n",
      "109/180\n",
      "110/180\n",
      "111/180\n",
      "112/180\n",
      "113/180\n",
      "114/180\n",
      "115/180\n",
      "116/180\n",
      "117/180\n",
      "118/180\n",
      "119/180\n",
      "120/180\n",
      "121/180\n",
      "122/180\n",
      "123/180\n",
      "124/180\n",
      "125/180\n",
      "126/180\n",
      "127/180\n",
      "128/180\n",
      "129/180\n",
      "130/180\n",
      "131/180\n",
      "132/180\n",
      "133/180\n",
      "134/180\n",
      "135/180\n",
      "136/180\n",
      "137/180\n",
      "138/180\n",
      "139/180\n",
      "140/180\n",
      "141/180\n",
      "142/180\n",
      "143/180\n",
      "144/180\n",
      "145/180\n",
      "146/180\n",
      "147/180\n",
      "148/180\n",
      "149/180\n",
      "150/180\n",
      "151/180\n",
      "152/180\n",
      "153/180\n",
      "154/180\n",
      "155/180\n",
      "156/180\n",
      "157/180\n",
      "158/180\n",
      "159/180\n",
      "160/180\n",
      "161/180\n",
      "162/180\n",
      "163/180\n",
      "164/180\n",
      "165/180\n",
      "166/180\n",
      "167/180\n",
      "168/180\n",
      "169/180\n",
      "170/180\n",
      "171/180\n",
      "172/180\n",
      "173/180\n",
      "174/180\n",
      "175/180\n",
      "176/180\n",
      "177/180\n",
      "178/180\n",
      "179/180\n",
      "180/180\n",
      "181/180\n"
     ]
    }
   ],
   "source": [
    "batch_size = 80\n",
    "res = []\n",
    "for ind in range((len(df) // batch_size) + 1):\n",
    "    print(f'{ind + 1}/{(len(df) // batch_size)}')\n",
    "    comments = df.iloc[ind * batch_size:(ind + 1) * batch_size,0].tolist()\n",
    "\n",
    "    encoded_input = tokenizer(comments, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    encoded_input = {k: v.to('cuda') for k, v in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    res.append(sentence_embeddings.detach().cpu())\n",
    "\n",
    "res2 = torch.cat(res)\n",
    "res2 = res2.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "idf_train, idf_val, y_train, y_val = train_test_split(res2, df['toxic'], test_size=0.2)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1)\n",
    "model.fit(idf_train, y_train)\n",
    "res = model.predict_proba(idf_val)\n",
    "\n",
    "res_ = [[int(i > drop_rate/100) for i in res[:,1]] for drop_rate in range(0, 100)]\n",
    "\n",
    "# качество намного лучше чем у tf idf \n",
    "max(map(lambda x: accuracy_score(y_val, x), res_)), max(map(lambda x: f1_score(y_val, x), res_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf2d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39, 0.9004509191814083, 0.8565717141429285]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for drop_rate in range(100):\n",
    "    drop_rate /= 100\n",
    "    res_ = [int(i > drop_rate) for i in res[:,1]]\n",
    "    scores.append([drop_rate, accuracy_score(y_val, res_), f1_score(y_val, res_)])\n",
    "max(scores, key=lambda x: x[2]) # 0.42 best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e403186f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
